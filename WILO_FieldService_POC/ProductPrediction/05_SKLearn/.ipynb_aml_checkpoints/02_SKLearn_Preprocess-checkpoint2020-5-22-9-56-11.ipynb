{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, Dataset\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.estimator import Estimator\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment cfg\n",
    "with open(\"experiment_cfg.json\", \"r\") as cfg_file:\n",
    "    cfg = json.load(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipelines(cfg):\n",
    "    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # combine features\n",
    "    feat_union = FeatureUnion([\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    # preprocess all features\n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for multi target cols\n",
    "    multi_target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['multi_target_cols'], str)),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for numerical target cols\n",
    "    num_target_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_target_cols'], float))\n",
    "    ])\n",
    "    \n",
    "    all_target_pipe = FeatureUnion([\n",
    "        ('num_targets', num_target_pipe),\n",
    "        ('multi_targets', multi_target_pipe)\n",
    "    ])\n",
    "\n",
    "    return { 'feature_pipe': all_feat_pipe, 'target_pipe': all_target_pipe }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/preprocess.py\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pipe import create_pipelines\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--input', dest='prepared_data')\n",
    "parser.add_argument('--output', dest='preprocessed_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load datasets\n",
    "if args.prepared_data:\n",
    "    df = pd.read_csv(args.prepared_data + '/prepared_data.csv', sep=';', header=0)\n",
    "else:\n",
    "    df = run.input_datasets['df_prepared'].to_pandas_dataframe()\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # split data (test data from last t_test years)\n",
    "# t_test = 0.5\n",
    "# df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "# df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # select columns for training\n",
    "# cfg = {}\n",
    "# cfg['multi_cols'] = ['Symptoms']\n",
    "# cfg['num_target_cols'] = ['duration']\n",
    "# cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "# # create pipeline\n",
    "# pipelines = create_pipelines(cfg)\n",
    "\n",
    "# # fit pipelines and transform data\n",
    "# X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "# y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "# X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "# y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # rename columns\n",
    "# feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "# target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "# df_train = pd.concat([\n",
    "#     pd.DataFrame(X_train, columns=feature_columns),\n",
    "#     pd.DataFrame(y_train, columns=target_columns)\n",
    "# ], axis=1)\n",
    "\n",
    "# df_test = pd.concat([\n",
    "#     pd.DataFrame(X_test, columns=feature_columns),\n",
    "#     pd.DataFrame(y_test, columns=target_columns)\n",
    "# ], axis=1)\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # save train and test data\n",
    "# path = args.preprocessed_data if args.preprocessed_data else './outputs'\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# df_train.to_csv(path + '/train_data.csv', sep=';', header=True, index=False)\n",
    "# df_test.to_csv(path + '/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "# # save pipelines\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "# joblib.dump(pipelines, './outputs/pipelines.pkl')\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['Symptoms']\n",
    "cfg['num_target_cols'] = ['duration']\n",
    "cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# split datasets into groups per productid (and keep only with more than n entries)\n",
    "n = 20\n",
    "df_prods = [ df[df['ProductId']==prod].reset_index(drop=True) for prod in df['ProductId'].unique() if len(df[df['ProductId']==prod])>=n]\n",
    "\n",
    "# prepare data for each productid and save in dictionary\n",
    "prod_pipes = {}\n",
    "prod_train_data = {}\n",
    "prod_test_data = {}\n",
    "\n",
    "for i in range(len(df_prods)):\n",
    "    \n",
    "    prodid = df_prods[i]['ProductId'][0]\n",
    "   \n",
    "    ##############################################################################\n",
    "\n",
    "    # train test split\n",
    "    # get n% newest rows as test\n",
    "    n = 25\n",
    "    df_train = df_prods[i].sort_values(by=['Start'], ascending=False).iloc[int(len(df_prods[i])*((n)/100)):]\n",
    "    df_test = df_prods[i].sort_values(by=['Start'], ascending=False).iloc[:int(len(df_prods[i])*((n)/100))]\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    # create pipeline\n",
    "    pipelines = create_pipelines(cfg)\n",
    "\n",
    "    # fit pipelines and transform data\n",
    "    X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "    y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "    X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "    y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    # rename columns\n",
    "    feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "    target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "    ##############################################################################\n",
    "    \n",
    "    df_train = pd.concat([\n",
    "        pd.DataFrame(X_train, columns=feature_columns),\n",
    "        pd.DataFrame(y_train, columns=target_columns)\n",
    "    ], axis=1)\n",
    "\n",
    "    df_test = pd.concat([\n",
    "        pd.DataFrame(X_test, columns=feature_columns),\n",
    "        pd.DataFrame(y_test, columns=target_columns)\n",
    "    ], axis=1)\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    prod_pipes[prodid] = pipelines\n",
    "    prod_train_data[prodid] = df_train\n",
    "    prod_test_data[prodid] = df_test\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# save train and test data\n",
    "path = args.preprocessed_data if args.preprocessed_data else './outputs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "# df_train.to_csv(path + '/train_data.csv', sep=';', header=True, index=False)\n",
    "# df_test.to_csv(path + '/test_data.csv', sep=';', header=True, index=False)\n",
    "joblib.dump(prod_train_data, path + '/train_data.pkl')\n",
    "joblib.dump(prod_test_data, path + '/test_data.pkl')\n",
    "\n",
    "# save pipelines\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(prod_pipes, './outputs/pipelines.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(entry_script='preprocess.py', source_directory='src', \n",
    "              inputs=[ws.datasets[cfg['prepared_data_dataset']].as_named_input('df_prepared')],\n",
    "              compute_target='local', environment_definition=ws.environments[cfg['env_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: SKLearnPrediction_1592571216_c7271b2a\n",
      "Web View: https://ml.azure.com/experiments/SKLearnPrediction/runs/SKLearnPrediction_1592571216_c7271b2a?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-19T12:53:39.587505\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 10\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ preprocess.py ] with arguments: []\n",
      "After variable expansion, calling script [ preprocess.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 10\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.22347164154052734 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: SKLearnPrediction_1592571216_c7271b2a\n",
      "Web View: https://ml.azure.com/experiments/SKLearnPrediction/runs/SKLearnPrediction_1592571216_c7271b2a?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'SKLearnPrediction_1592571216_c7271b2a',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-19T12:53:38.891548Z',\n",
       " 'endTimeUtc': '2020-06-19T12:55:56.043223Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': 'bff5e8c5-4a12-43fe-b0f9-bf3589f1742b'},\n",
       " 'inputDatasets': [{'dataset': {'id': '79c10931-897b-434d-b889-8469c14e6556'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'df_prepared', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'preprocess.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'df_prepared': {'dataLocation': {'dataset': {'id': '79c10931-897b-434d-b889-8469c14e6556',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'df_prepared',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'WILO_POC_sklearn',\n",
       "   'version': '2',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults',\n",
       "        'azureml-dataprep[pandas,fuse]',\n",
       "        'pyarrow==0.12.0',\n",
       "        'joblib==0.14.1',\n",
       "        'scikit-learn==0.20.3']},\n",
       "      'numpy==1.16.2'],\n",
       "     'name': 'azureml_42df74e95cf2de1f301b9fba9e8035c0'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.SKLearnPrediction_1592571216_c7271b2a/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=GYx5mMdnlg5MqSnAVSDQp1NKhOFk8VvvIR1dKf4Kkw4%3D&st=2020-06-19T12%3A46%3A06Z&se=2020-06-19T20%3A56%3A06Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.SKLearnPrediction_1592571216_c7271b2a/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=Uf1y4PwNbrW6pFdHec7GRLhZ7BEWxqGEDx2PSxYSTgw%3D&st=2020-06-19T12%3A46%3A06Z&se=2020-06-19T20%3A56%3A06Z&sp=r',\n",
       "  'logs/azureml/10_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.SKLearnPrediction_1592571216_c7271b2a/logs/azureml/10_azureml.log?sv=2019-02-02&sr=b&sig=b7JJZX%2F2NSSNyMQc5O9MPVxCzVcGXb%2FyFbaHSqI%2FfM0%3D&st=2020-06-19T12%3A46%3A06Z&se=2020-06-19T20%3A56%3A06Z&sp=r'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(ws, cfg['experiment_name'])\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading artifacts/train_data.pkl\n",
      "Uploaded artifacts/train_data.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', './SKLearnPrediction')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"b29d3ea3-3f43-40d0-82cd-c2f7b71cee28\",\n",
       "    \"name\": \"SKLearnTrainData\",\n",
       "    \"version\": 2,\n",
       "    \"workspace\": \"Workspace.create(name='resdynml1test', subscription_id='793146d9-d4dc-4a73-9728-76c4ffd0cc0d', resource_group='rg_dynamics_test')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.download_file('outputs/train_data.pkl', output_file_path='artifacts/train_data.pkl')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/train_data.pkl'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "#prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset = Dataset.File.from_files(path=data_ref)\n",
    "prepared_data_dataset.register(ws, cfg['train_dataset'], create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading artifacts/test_data.pkl\n",
      "Uploaded artifacts/test_data.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', './SKLearnPrediction')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"d154df9d-5eb2-4a20-83fa-3e398ccfbcb1\",\n",
       "    \"name\": \"SKLearnTestData\",\n",
       "    \"version\": 2,\n",
       "    \"workspace\": \"Workspace.create(name='resdynml1test', subscription_id='793146d9-d4dc-4a73-9728-76c4ffd0cc0d', resource_group='rg_dynamics_test')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.download_file('outputs/test_data.pkl', output_file_path='artifacts/test_data.pkl')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/test_data.pkl'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "# prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset = Dataset.File.from_files(path=data_ref)\n",
    "prepared_data_dataset.register(ws, cfg['test_dataset'], create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Pipelines (as Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='resdynml1test', subscription_id='793146d9-d4dc-4a73-9728-76c4ffd0cc0d', resource_group='rg_dynamics_test'), name=SKLearnPreprocessPipeline, id=SKLearnPreprocessPipeline:2, version=2, tags={}, properties={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.register_model(cfg['PreprocessPipeline'], 'outputs/pipelines.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
