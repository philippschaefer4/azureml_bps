{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, Dataset\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.estimator import Estimator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment cfg\n",
    "with open(\"experiment_cfg.json\", \"r\") as cfg_file:\n",
    "    cfg = json.load(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/prepare.py\n",
    "\n",
    "from azureml.core import Run\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--output', dest='prepared_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df_raw'].to_pandas_dataframe()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "t = 5\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum())))),\n",
    "    'Start': x['Job Card.Date Start Work'].min(),\n",
    "    'End': x['Job Card.Date End Work'].max()\n",
    "  })).reset_index()\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# compute duration column\n",
    "df = pd.concat([df, pd.DataFrame((df['End'] - df['Start']), columns=['duration'])],axis=1)\n",
    "df['duration'] = df['duration'].apply(lambda x: x.seconds / 3600)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# save train and test data\n",
    "path = arg.prepared_data if args.prepared_data else './outputs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "df.to_csv(path + '/prepared_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(entry_script='prepare.py', source_directory='src', \n",
    "              inputs=[   ws.datasets[cfg['symptomcodes_dataset']].as_named_input('symptomcodes'), \n",
    "                         ws.datasets[cfg['raw_data_dataset']].as_named_input('df_raw')       ],\n",
    "              compute_target=ws.compute_targets['mlcompute'], environment_definition=ws.environments[cfg['env_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, cfg['experiment_name'])\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/prepared_data.csv', output_file_path='artifacts/prepared_data.csv')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/prepared_data.csv'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset.register(ws, cfg['prepared_data_dataset'], create_new_version=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
