{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipeline(cfg):    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for target features\n",
    "    target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['target_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "#   # Pipeline for categories\n",
    "#     cat_pipe = Pipeline([\n",
    "#         ('cat_feature_select', DataFrameSelector(cfg['cat_cols'])),\n",
    "#         ('cat_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='0')),\n",
    "#         ('cat_one_hot_encode', OneHotEncoder(sparse=False))\n",
    "#     ])\n",
    "\n",
    "#     # Pipeline for numericals\n",
    "#     num_pipe = Pipeline([\n",
    "#         ('num_feature_select', DataFrameSelector(cfg['num_cols'])),\n",
    "#         ('num_replace_missing', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "#         #('num_normalization', MinMaxScaler())\n",
    "#         ('num_standardization', StandardScaler())\n",
    "#     ])\n",
    "\n",
    "    feat_union = FeatureUnion([\n",
    "#         ('num_features', num_pipe),\n",
    "#         ('cat_features', cat_pipe),\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "        ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    pipeline = FeatureUnion([\n",
    "        (\"all_feat_pipe\", all_feat_pipe),\n",
    "        (\"target_pipe\", target_pipe)\n",
    "    ])\n",
    "\n",
    "    return pipeline    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "run.log('# rows before', len(df))\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "t = 2\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "run.log('# rows for last ' +str(t) + ' years', len(df))\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "run.log('# rows after cleaning', len(df))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "#     'ProductGroup': tuple(x['Installed Base.Product Group'].unique()),\n",
    "    'ProductGroup': ' '.join(x['Installed Base.Product Group'].unique()),\n",
    "#     'ProductId': tuple(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'ProductId': ' '.join(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'Country': x['Location.Country'].unique()[0],\n",
    "    'City': x['Location.City'].unique()[0],\n",
    "    'LocationType': x['Location.Location Type'].unique()[0],\n",
    "    'PostalCode': x['Location.Postal Code'].unique()[0],\n",
    "#     'ProductName': tuple(x['Product.Product Name'].unique()), \n",
    "    'ProductName': ' '.join(x['Product.Product Name'].unique()), \n",
    "#     'ProductNr': tuple(x['Product.Product Number'].unique()),\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "#     'Quantity': tuple((x['Product.Product Number']),x['ItemResourceAppliedQuantity']),\n",
    "    'Start': x['Job Card.Date Start Work'].unique()[0],\n",
    "    'End': x['Job Card.Date End Work'].unique()[0],\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum()))))\n",
    "  })).reset_index()\n",
    "\n",
    "run.log('# rows after merging cases', len(df))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "t_test = 0.5\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "run.log('# rows in train data', len(df_train))\n",
    "run.log('# rows in test data', len(df_test))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['ProductGroup', 'Symptoms'] #['ProductGroup', 'ProductId', 'Symptoms']\n",
    "cfg['cat_cols'] = ['Country', 'City', 'LocationType', 'PostalCode']\n",
    "cfg['date_cols'] = ['Start', 'End']\n",
    "cfg['num_cols'] = []\n",
    "cfg['target_cols'] = ['ProductNr']\n",
    "\n",
    "# create pipeline\n",
    "pipe = create_pipeline(cfg)\n",
    "\n",
    "# transform data\n",
    "df_train = pipe.fit_transform(df_train)\n",
    "df_test = pipe.transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "columns = [ 'feat_' + str(i) if i < df_train.shape[1]-len(pipe.transformer_list[1][1].named_steps['target_encode'].col_cats[0]) else 'target_' + str(i) for i in range(df_train.shape[1]) ]\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_test = pd.DataFrame(df_test, columns=columns)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# save prepared data to csv\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SKLearn(entry_script='prep.py', source_directory='src', \n",
    "              inputs=[   ws.datasets['symptomcodes.csv'].as_named_input('symptomcodes'), \n",
    "                         ws.datasets['ItemResourceData.csv'].as_named_input('df')       ],\n",
    "              pip_packages=['pyarrow==0.12.0 '], compute_target='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: ProductPrediction_1591974819_8f3aaf84\n",
      "Web View: https://ml.azure.com/experiments/ProductPrediction/runs/ProductPrediction_1591974819_8f3aaf84?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-12T15:13:42.193980\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ prep.py ] with arguments: []\n",
      "After variable expansion, calling script [ prep.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.42168569564819336 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: ProductPrediction_1591974819_8f3aaf84\n",
      "Web View: https://ml.azure.com/experiments/ProductPrediction/runs/ProductPrediction_1591974819_8f3aaf84?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'ProductPrediction_1591974819_8f3aaf84',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-12T15:13:41.302125Z',\n",
       " 'endTimeUtc': '2020-06-12T15:26:39.941816Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': 'c77142cf-4715-4974-88c7-a2f5b7a955ba'},\n",
       " 'inputDatasets': [{'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'df', 'mechanism': 'Direct'}}, {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'symptomcodes', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'prep.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'df': {'dataLocation': {'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'df',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False},\n",
       "   'symptomcodes': {'dataLocation': {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'symptomcodes',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment ProductPrediction Environment',\n",
       "   'version': 'Autosave_2020-06-12T13:31:47Z_23569664',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['pyarrow==0.12.0',\n",
       "        'azureml-defaults',\n",
       "        'scikit-learn==0.20.3',\n",
       "        'scipy==1.2.1',\n",
       "        'numpy==1.16.2',\n",
       "        'joblib==0.13.2']}],\n",
       "     'name': 'azureml_6da8db82c0a6a27195a6a6ae29218268'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPrediction_1591974819_8f3aaf84/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=u0H5A%2FyKLAPm%2BMe2r5LDqtzNUvSIKUK292Tqjrk%2BvgU%3D&st=2020-06-12T15%3A16%3A40Z&se=2020-06-12T23%3A26%3A40Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPrediction_1591974819_8f3aaf84/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=AXviW1kpOzmwH6aDdX%2B9HmtSSQBXvcGIHxvyBjXrOJE%3D&st=2020-06-12T15%3A16%3A40Z&se=2020-06-12T23%3A26%3A40Z&sp=r',\n",
       "  'logs/azureml/9_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPrediction_1591974819_8f3aaf84/logs/azureml/9_azureml.log?sv=2019-02-02&sr=b&sig=oyIW5eNqYqkQCPsiQxRS0ZeqbXh%2F%2BIi9mATGnbCsnPo%3D&st=2020-06-12T15%3A16%3A40Z&se=2020-06-12T23%3A26%3A40Z&sp=r'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(ws, 'ProductPrediction')\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
