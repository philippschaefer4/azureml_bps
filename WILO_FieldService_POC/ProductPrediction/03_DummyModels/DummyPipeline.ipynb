{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.sklearn import SKLearn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipelines(cfg):\n",
    "    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # combine features\n",
    "    feat_union = FeatureUnion([\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    # preprocess all features\n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for multi target cols\n",
    "    multi_target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['multi_target_cols'], str)),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for numerical target cols\n",
    "    num_target_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_target_cols'], float))\n",
    "    ])\n",
    "    \n",
    "    all_target_pipe = FeatureUnion([\n",
    "        ('num_targets', num_target_pipe),\n",
    "        ('multi_targets', multi_target_pipe)\n",
    "    ])\n",
    "\n",
    "    return all_feat_pipe, all_target_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipelines\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "t = 0.5\n",
    "t_test = 0.1\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--output', dest='prepared_data')\n",
    "parser.add_argument('--pipeline_data', dest='pipeline_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum())))),\n",
    "    'Start': x['Job Card.Date Start Work'].min(),\n",
    "    'End': x['Job Card.Date End Work'].max()\n",
    "  })).reset_index()\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame((df['End'] - df['Start']), columns=['duration'])],axis=1)\n",
    "df['duration'] = df['duration'].apply(lambda x: x.seconds / 3600)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['Symptoms']\n",
    "cfg['num_target_cols'] = ['duration']\n",
    "cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "feature_pipe, target_pipe = create_pipelines(cfg)\n",
    "pipelines = { 'feature_pipe': feature_pipe, 'target_pipe': target_pipe }\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "df_train = pd.concat([\n",
    "    pd.DataFrame(X_train, columns=feature_columns),\n",
    "    pd.DataFrame(y_train, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    pd.DataFrame(X_test, columns=feature_columns),\n",
    "    pd.DataFrame(y_test, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# save train and test data to run output\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "# and save train and test data to PipelineData output\n",
    "os.makedirs(args.prepared_data, exist_ok=True)\n",
    "df_train.to_csv(args.prepared_data + '/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv(args.prepared_data + '/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "# save pipelines only in run output\n",
    "joblib.dump(pipelines, './outputs/pipelines.pkl')\n",
    "\n",
    "# and save in PipelineData output\n",
    "joblib.dump(pipelines, args.pipeline_data)# + '/pipelines.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import recall_score, precision_score, hamming_loss, zero_one_loss, mean_absolute_error, mean_squared_error, r2_score\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--input', dest='prepared_data')\n",
    "parser.add_argument('--trained_classifier', dest='trained_classifier')\n",
    "parser.add_argument('--trained_regressor', dest='trained_regressor')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load data\n",
    "# train_data = run.input_datasets['train_data'].to_pandas_dataframe()\n",
    "# test_data = run.input_datasets['test_data'].to_pandas_dataframe()\n",
    "file_path = args.prepared_data\n",
    "train_data = pd.read_csv(file_path + '/train_data.csv', sep=';', header=0)\n",
    "test_data = pd.read_csv(file_path + '/test_data.csv', sep=';', header=0)\n",
    "\n",
    "# split train/test and feat/target\n",
    "X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "\n",
    "############################################################\n",
    "\n",
    "# train classifier\n",
    "model = MultiOutputClassifier(DummyClassifier(strategy='stratified'))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate test data\n",
    "y_pred = model.predict(X_test)\n",
    "run.log('precision_macro', precision_score(y_test, y_pred, average='macro'))\n",
    "run.log('precision_samples', precision_score(y_test, y_pred, average='samples'))\n",
    "run.log('recall_macro', recall_score(y_test, y_pred, average='macro'))\n",
    "run.log('recall_samples', recall_score(y_test, y_pred, average='samples'))\n",
    "run.log('hamming_loss', hamming_loss(y_test, y_pred))\n",
    "run.log('zero_one_loss', zero_one_loss(y_test, y_pred))\n",
    "\n",
    "# evaluate train data\n",
    "y_pred = model.predict(X_train)\n",
    "run.log('precision_macro_train', precision_score(y_train, y_pred, average='macro'))\n",
    "run.log('precision_samples_train', precision_score(y_train, y_pred, average='samples'))\n",
    "run.log('recall_macro_train', recall_score(y_train, y_pred, average='macro'))\n",
    "run.log('recall_samples_train', recall_score(y_train, y_pred, average='samples'))\n",
    "run.log('hamming_loss_train', hamming_loss(y_train, y_pred))\n",
    "run.log('zero_one_loss_train', zero_one_loss(y_train, y_pred))\n",
    "\n",
    "# save model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "joblib.dump(value=model, filename= args.trained_classifier)# + '/model.pkl')\n",
    "\n",
    "############################################################\n",
    "\n",
    "# train regressor\n",
    "X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]][['target_0']]\n",
    "X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]][['target_0']]\n",
    "\n",
    "model_regressor = DummyRegressor(strategy=\"mean\")\n",
    "model_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_regressor.predict(X_test)\n",
    "run.log('mae', mean_absolute_error(y_test, y_pred))\n",
    "run.log('mse', mean_squared_error(y_test, y_pred))\n",
    "run.log('r2', r2_score(y_test, y_pred))\n",
    "\n",
    "y_pred = model_regressor.predict(X_train)\n",
    "run.log('mae_train', mean_absolute_error(y_train, y_pred))\n",
    "run.log('mse_train', mean_squared_error(y_train, y_pred))\n",
    "run.log('r2_train', r2_score(y_train, y_pred))\n",
    "\n",
    "# save regressor model\n",
    "joblib.dump(value=model_regressor, filename='outputs/model_regressor.pkl')\n",
    "joblib.dump(value=model_regressor, filename=args.trained_regressor)# + '/model_regressor.pkl')\n",
    "\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deploy.py\n",
    "\n",
    "from azureml.core import Run, Model\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--pipeline_data', dest='pipeline_data')\n",
    "parser.add_argument('--trained_classifier', dest='trained_classifier')\n",
    "parser.add_argument('--trained_regressor', dest='trained_regressor')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Model.register(args.pipeline_data, 'DummyPipe', ws)\n",
    "# Model.register(args.trained_classifier, 'DummyModel', ws)\n",
    "# Model.register(args.trained_regressor, 'DummyModelRegressor', ws)\n",
    "\n",
    "for child in run.parent.get_children():\n",
    "    if child.name == 'prep.py':\n",
    "        child.register_model('DummyPipe', 'outputs/pipelines.pkl')\n",
    "    elif child.name == 'train.py':\n",
    "        child.register_model('DummyModel', 'outputs/model.pkl')\n",
    "        child.register_model('DummyModelRegressor', 'outputs/model_regressor.pkl')\n",
    "        \n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/score.py\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "from pipe import create_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global regressor\n",
    "    global pipelines\n",
    "    model_path = Model.get_model_path('DummyModel')\n",
    "    model = joblib.load(model_path)\n",
    "    regressor_path = Model.get_model_path('DummyModelRegressor')\n",
    "    regressor = joblib.load(regressor_path)\n",
    "    pipeline_path = Model.get_model_path('DummyPipe')\n",
    "    pipelines = joblib.load(pipeline_path)\n",
    "    \n",
    "def run(raw_data):\n",
    "    \n",
    "    # get input data\n",
    "    data = json.loads(raw_data)\n",
    "    \n",
    "    # transform with pipeline\n",
    "    X = pipelines['feature_pipe'].transform(pd.DataFrame(data))\n",
    "    \n",
    "    # make prediction\n",
    "    y = model.predict(X)\n",
    "    \n",
    "    # predict duration\n",
    "    y_dur = regressor.predict(X)\n",
    "    \n",
    "    response = [\n",
    "        {\n",
    "            'Products':\n",
    "            [ \n",
    "                pipelines['target_pipe'].transformer_list[1][1].named_steps['target_encode'].col_cats[0][i] \n",
    "                for i in range(y.shape[1]) if y[j,i] == 1 \n",
    "            ],\n",
    "            'Duration':\n",
    "                 y_dur[j,0]\n",
    "        }        \n",
    "            for j in range(y.shape[0])\n",
    "    ]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dependencies\n",
    "\n",
    "# cd = CondaDependencies()\n",
    "# cd.add_pip_package(\"azureml-defaults\")\n",
    "# cd.add_pip_package('pyarrow==0.12.0')\n",
    "# cd.add_pip_package('joblib')\n",
    "# cd.add_pip_package('scikit-learn==0.20.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register environment\n",
    "\n",
    "# if 'WILO_POC' in ws.environments:\n",
    "#     env = ws.environments['WILO_POC']\n",
    "# else:\n",
    "#     env = Environment('WILO_POC')\n",
    "# env.python.conda_dependencies = cd\n",
    "# env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run config\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = ws.environments['WILO_POC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DataObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "symptoms_data = ws.datasets['symptomcodes.csv'].as_named_input('symptomcodes')\n",
    "raw_input_data = ws.datasets['ItemResourceData.csv'].as_named_input('df')\n",
    "# prepared data\n",
    "prepared_data = PipelineData(\"prepared_data\", datastore=ws.datastores['workspaceblobstore'], is_directory=True)\n",
    "\n",
    "# output\n",
    "pipeline_data = PipelineData(\"pipeline_data\", datastore=ws.datastores['workspaceblobstore'], is_directory=False)\n",
    "trained_classifier = PipelineData(\"trained_classifier\", datastore=ws.datastores['workspaceblobstore'], is_directory=False)\n",
    "trained_regressor = PipelineData(\"trained_regressor\", datastore=ws.datastores['workspaceblobstore'], is_directory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_step = PythonScriptStep(script_name='prep.py', source_directory='src',\n",
    "                            inputs=[symptoms_data, raw_input_data], outputs=[pipeline_data, prepared_data],\n",
    "                             arguments=['--pipeline_data', pipeline_data, '--output', prepared_data],\n",
    "                            compute_target=ws.compute_targets['mlcompute'], runconfig=run_config, allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(script_name='train.py', source_directory='src',\n",
    "                             inputs=[prepared_data], outputs=[trained_classifier, trained_regressor],\n",
    "                             arguments=['--input', prepared_data, '--trained_classifier', trained_classifier, '--trained_regressor', trained_regressor],\n",
    "                             compute_target=ws.compute_targets['mlcompute'], runconfig=run_config, allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_step = PythonScriptStep(script_name='deploy.py', source_directory='src',\n",
    "                               inputs=[pipeline_data, trained_classifier, trained_regressor],\n",
    "                               arguments=['--pipeline_data', pipeline_data, '--trained_classifier', trained_classifier, '--trained_regressor', trained_regressor],\n",
    "                             compute_target=ws.compute_targets['mlcompute'], runconfig=run_config, allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, [prep_step, train_step, deploy_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step prep.py [6a3546b0][89473af0-956d-4ac8-825a-0b47c37e520c], (This step will run and generate new outputs)\n",
      "Created step train.py [9331e624][c6a7951d-7d3d-4a4c-9ce8-4cce2a21c4d8], (This step will run and generate new outputs)\n",
      "Created step deploy.py [2ce0593a][46d94c06-f4b6-4b5b-8dbd-6ed5d88c0219], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun a937fb2b-34da-41b8-bdb1-f87a65310e19\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/DummyPrediction/runs/a937fb2b-34da-41b8-bdb1-f87a65310e19?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "PipelineRunId: a937fb2b-34da-41b8-bdb1-f87a65310e19\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/DummyPrediction/runs/a937fb2b-34da-41b8-bdb1-f87a65310e19?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: be472185-fea5-4b73-aa78-e63523082f11\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/DummyPrediction/runs/be472185-fea5-4b73-aa78-e63523082f11?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "StepRun( prep.py ) Status: NotStarted\n",
      "StepRun( prep.py ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "========================================================================================================================\n",
      "2020-06-17T18:28:27Z Starting output-watcher...\n",
      "2020-06-17T18:28:27Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "a8a2ed0772c4330b283bc67d44f8d47789873575fb65fe08990bfa71ca169124\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "===============================================================================================================\n",
      "Entering job preparation. Current time:2020-06-17T18:29:17.499581\n",
      "Starting job preparation. Current time:2020-06-17T18:29:18.063024\n",
      "Extracting the control code.\n",
      "fetching and extracting the control code on master node.\n",
      "Retrieving project from snapshot: 4756cf09-388e-4931-8596-1d5adec5e77b\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 56\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "downloadDataStore - Download from datastores if requested.\n",
      "Entering context manager injector. Current time:2020-06-17T18:29:19.795980\n",
      "Acquired lockfile /tmp/be472185-fea5-4b73-aa78-e63523082f11-datastore.lock to downloading input data references\n",
      "downloadDataStore completed\n",
      "Job preparation is complete. Current time:2020-06-17T18:29:20.853295\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "Entering context manager injector. Current time:2020-06-17T18:29:21.929491\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 117\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ prep.py ] with arguments: ['--pipeline_data', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/be472185-fea5-4b73-aa78-e63523082f11/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/pipeline_data', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/be472185-fea5-4b73-aa78-e63523082f11/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data']\n",
      "After variable expansion, calling script [ prep.py ] with arguments: ['--pipeline_data', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/be472185-fea5-4b73-aa78-e63523082f11/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/pipeline_data', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/be472185-fea5-4b73-aa78-e63523082f11/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data']\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 117\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.4254181385040283 seconds\n",
      "2020/06/17 18:30:34 Process Exiting with Code:  0\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "===============================================================================================================\n",
      "Entering job release. Current time:2020-06-17T18:30:34.877780\n",
      "Starting job release. Current time:2020-06-17T18:30:35.786346\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 333\n",
      "Entering context manager injector. Current time:2020-06-17T18:30:35.805140\n",
      "Job release is complete. Current time:2020-06-17T18:30:37.791668\n",
      "\n",
      "StepRun(prep.py) Execution Summary\n",
      "===================================\n",
      "StepRun( prep.py ) Status: Finished\n",
      "{'runId': 'be472185-fea5-4b73-aa78-e63523082f11', 'target': 'mlcompute', 'status': 'Completed', 'startTimeUtc': '2020-06-17T18:28:22.746685Z', 'endTimeUtc': '2020-06-17T18:30:40.20701Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '4756cf09-388e-4931-8596-1d5adec5e77b', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '89473af0-956d-4ac8-825a-0b47c37e520c', 'azureml.pipelinerunid': 'a937fb2b-34da-41b8-bdb1-f87a65310e19', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'symptomcodes', 'mechanism': 'Direct'}}, {'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'df', 'mechanism': 'Direct'}}], 'runDefinition': {'script': 'prep.py', 'useAbsolutePath': False, 'arguments': ['--pipeline_data', '$AZUREML_DATAREFERENCE_pipeline_data', '--output', '$AZUREML_DATAREFERENCE_prepared_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mlcompute', 'dataReferences': {'pipeline_data': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/be472185-fea5-4b73-aa78-e63523082f11/pipeline_data', 'pathOnCompute': None, 'overwrite': False}, 'prepared_data': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'symptomcodes': {'dataLocation': {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909', 'name': None, 'version': None}, 'dataPath': None}, 'mechanism': 'Direct', 'environmentVariableName': 'symptomcodes', 'pathOnCompute': None, 'overwrite': False}, 'df': {'dataLocation': {'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c', 'name': None, 'version': None}, 'dataPath': None}, 'mechanism': 'Direct', 'environmentVariableName': 'df', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment DummyPrediction Environment', 'version': 'Autosave_2020-06-17T11:49:17Z_8e1b2bf8', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults', 'pyarrow==0.12.0', 'joblib', 'scikit-learn==0.20.3']}], 'name': 'azureml_55f779a3e51294cd60bb1901cf850cf6'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '1g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'itpCompute': {'configuration': {}}, 'cmAksCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=6nTnOzHL8aF76XLvjYGAuoO5IqwgBQOAx8hzephqi7Q%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=dE0w7sQwelxuVvZt%2FJILUJAFTvzjF%2Fb5Ghjf9SouzZw%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=2RhM9jYZ%2FqJKcR3arVsVXFMaL28U95ECHYxkhtxdxV8%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=EV429RDWo7foEhsNajG1VOxDKxlsXHJtwczc%2FUUk8VQ%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'azureml-logs/process_info.json': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=2RVsGXoGuQ2CiNLFgInGTJ%2BYc46L537uWcSphLtDLpI%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'azureml-logs/process_status.json': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Boua%2BHPujSCsTbbd%2FmTZy8tXjA%2BJjx%2FvkwM%2FcwuXzB4%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/117_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/117_azureml.log?sv=2019-02-02&sr=b&sig=jsGeSmi99%2BFRthASnnEIC2DIWqaiaRlpW9rrXQqzEW4%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=pYntDjU3BkqEwCHwQPj0k%2BgXRz%2Bdg77c6jK7v3yzVZU%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=kce2xQn1DS1TC3L3JQ%2F3Y0lBCiOgvA4tEmrbT45%2FPZM%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=ijn%2FS3C2OzQRd790lKrKg9iZEMOXa3R0sTzRI48K6kY%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=VOaq6WKQ9jtLcACBChU2ZsED9Ds%2FLTyzTcw4PdW8o94%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.be472185-fea5-4b73-aa78-e63523082f11/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=UJydXjc%2FpuGccK4wmnoboqy5C4qF684eTapVjk07gaQ%3D&st=2020-06-17T18%3A20%3A49Z&se=2020-06-18T02%3A30%3A49Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: fd63693c-909c-44d0-be1b-6497608e91fc\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/DummyPrediction/runs/fd63693c-909c-44d0-be1b-6497608e91fc?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "StepRun( train.py ) Status: NotStarted\n",
      "StepRun( train.py ) Status: Queued\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "========================================================================================================================\n",
      "2020-06-17T18:31:16Z Starting output-watcher...\n",
      "2020-06-17T18:31:16Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "df1513763a3d64d39928d67aa989f81ac8ac40c81f5936be86c9336d11d359c1\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "===============================================================================================================\n",
      "Entering job preparation. Current time:2020-06-17T18:31:18.433045\n",
      "StepRun( train.py ) Status: Running\n",
      "Starting job preparation. Current time:2020-06-17T18:31:19.118546\n",
      "Extracting the control code.\n",
      "fetching and extracting the control code on master node.\n",
      "Retrieving project from snapshot: 4756cf09-388e-4931-8596-1d5adec5e77b\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 56\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "downloadDataStore - Download from datastores if requested.\n",
      "Entering context manager injector. Current time:2020-06-17T18:31:20.947971\n",
      "Acquired lockfile /tmp/fd63693c-909c-44d0-be1b-6497608e91fc-datastore.lock to downloading input data references\n",
      "downloadDataStore completed\n",
      "Job preparation is complete. Current time:2020-06-17T18:31:22.010254\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "Entering context manager injector. Current time:2020-06-17T18:31:23.473642\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 118\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ train.py ] with arguments: ['--input', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data', '--trained_classifier', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_classifier', '--trained_regressor', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_regressor']\n",
      "After variable expansion, calling script [ train.py ] with arguments: ['--input', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data', '--trained_classifier', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_classifier', '--trained_regressor', '/mnt/batch/tasks/shared/LS_root/jobs/resdynml1test/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/mounts/workspaceblobstore/azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_regressor']\n",
      "\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt\n",
      "===============================================================================================================\n",
      "Entering job release. Current time:2020-06-17T18:31:49.917046\n",
      "Starting job release. Current time:2020-06-17T18:31:50.802525\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 197\n",
      "Entering context manager injector. Current time:2020-06-17T18:31:50.820775\n",
      "Job release is complete. Current time:2020-06-17T18:31:52.118971\n",
      "\n",
      "StepRun(train.py) Execution Summary\n",
      "====================================\n",
      "StepRun( train.py ) Status: Finished\n",
      "{'runId': 'fd63693c-909c-44d0-be1b-6497608e91fc', 'target': 'mlcompute', 'status': 'Completed', 'startTimeUtc': '2020-06-17T18:31:18.119386Z', 'endTimeUtc': '2020-06-17T18:31:54.985605Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '4756cf09-388e-4931-8596-1d5adec5e77b', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'c6a7951d-7d3d-4a4c-9ce8-4cce2a21c4d8', 'azureml.pipelinerunid': 'a937fb2b-34da-41b8-bdb1-f87a65310e19', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'runDefinition': {'script': 'train.py', 'useAbsolutePath': False, 'arguments': ['--input', '$AZUREML_DATAREFERENCE_prepared_data', '--trained_classifier', '$AZUREML_DATAREFERENCE_trained_classifier', '--trained_regressor', '$AZUREML_DATAREFERENCE_trained_regressor'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mlcompute', 'dataReferences': {'prepared_data': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/be472185-fea5-4b73-aa78-e63523082f11/prepared_data', 'pathOnCompute': None, 'overwrite': False}, 'trained_classifier': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_classifier', 'pathOnCompute': None, 'overwrite': False}, 'trained_regressor': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/fd63693c-909c-44d0-be1b-6497608e91fc/trained_regressor', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment DummyPrediction Environment', 'version': 'Autosave_2020-06-17T11:49:17Z_8e1b2bf8', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults', 'pyarrow==0.12.0', 'joblib', 'scikit-learn==0.20.3']}], 'name': 'azureml_55f779a3e51294cd60bb1901cf850cf6'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '1g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'itpCompute': {'configuration': {}}, 'cmAksCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/55_azureml-execution-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=SR6TOEaeSH6BacisCSRnSJODD2uw1V3zcQFmAc8K%2BuE%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/65_job_prep-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=UiKQ1%2F1hjQ1miGlmlZtx3D0RWRjjyTM06wnnWKak5y8%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=Wg%2BHO3oj6LbFB6v26yIVp0FoQjrVX4hnedEhSonXM9k%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/75_job_post-tvmps_2f1de38d79ebbfc529747028dee486c67ec69c7c0947d63ee4f3f8a59fef922a_d.txt?sv=2019-02-02&sr=b&sig=3XpuyVZuRKptXKjVyL7Xad1z0a7soAV8Npl2QlR3sLg%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'azureml-logs/process_info.json': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=mjsyyOeLq2IyUSeP7kJYYErTlHJaIZReWDjXBXfAroo%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'azureml-logs/process_status.json': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Uwu2Sq7cVvuzNO3WSCeCnnEtRfAy%2BsEccC43yaD74mo%3D&st=2020-06-17T18%3A22%3A02Z&se=2020-06-18T02%3A32%3A02Z&sp=r', 'logs/azureml/118_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/118_azureml.log?sv=2019-02-02&sr=b&sig=kXjEtxbbw5m9xuRrI5LCoKqRcFZhpbzBK1xZGZwbVnk%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=7EteuVIKfwdhw8OhJcHq7wCpWtCPsERTeioSEGshI%2B0%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=EvjMQm%2BNykq%2BcQ3oG7uDtEDi21JFD2091Gvg%2BxNOKq4%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=5qQpmEiiAb5j622TqqrZ5kV0ONU1b9gijWrPtkIgSjI%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=40dwRKHN9ev%2FqMElad7pGRkEhkhJssID6Fh8eSty8uw%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.fd63693c-909c-44d0-be1b-6497608e91fc/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ufofG2LcTkP8dUtcXws7glwiAxeqYrWHkaB%2FJJTLuKk%3D&st=2020-06-17T18%3A22%3A01Z&se=2020-06-18T02%3A32%3A01Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: 0551acac-5c23-486e-8edc-b75ececf4811\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/DummyPrediction/runs/0551acac-5c23-486e-8edc-b75ececf4811?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "StepRun( deploy.py ) Status: Queued\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(ws, 'DummyPrediction')\n",
    "run = exp.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'DummyPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in exp.get_runs():\n",
    "    run = r\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in run.get_children():\n",
    "    child = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
