{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.sklearn import SKLearn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "# %%writefile ./score/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipeline(cfg):    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for target features\n",
    "    target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['target_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "#   # Pipeline for categories\n",
    "#     cat_pipe = Pipeline([\n",
    "#         ('cat_feature_select', DataFrameSelector(cfg['cat_cols'])),\n",
    "#         ('cat_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='0')),\n",
    "#         ('cat_one_hot_encode', OneHotEncoder(sparse=False))\n",
    "#     ])\n",
    "\n",
    "    # Pipeline for numericals\n",
    "    num_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_cols'], float))\n",
    "#         ('num_replace_missing', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "#         #('num_normalization', MinMaxScaler())\n",
    "#         ('num_standardization', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    feat_union = FeatureUnion([\n",
    "#         ('num_features', num_pipe),\n",
    "#         ('cat_features', cat_pipe),\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    pipeline = FeatureUnion([\n",
    "        (\"all_feat_pipe\", all_feat_pipe),\n",
    "        ('num_targets', num_pipe),\n",
    "        (\"target_pipe\", target_pipe)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def create_pipelines(cfg):\n",
    "    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # combine features\n",
    "    feat_union = FeatureUnion([\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    # preprocess all features\n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for multi target cols\n",
    "    multi_target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['multi_target_cols'], str)),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for numerical target cols\n",
    "    num_target_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_target_cols'], float))\n",
    "    ])\n",
    "    \n",
    "    all_target_pipe = FeatureUnion([\n",
    "        ('num_targets', num_target_pipe),\n",
    "        ('multi_targets', multi_target_pipe)\n",
    "    ])\n",
    "\n",
    "    return all_feat_pipe, all_target_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipelines\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "t = 0.5\n",
    "t_test = 0.1\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum())))),\n",
    "    'Start': x['Job Card.Date Start Work'].min(),\n",
    "    'End': x['Job Card.Date End Work'].max()\n",
    "  })).reset_index()\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame((df['End'] - df['Start']), columns=['duration'])],axis=1)\n",
    "df['duration'] = df['duration'].apply(lambda x: x.seconds / 3600)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['Symptoms']\n",
    "cfg['num_target_cols'] = ['duration']\n",
    "cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "feature_pipe, target_pipe = create_pipelines(cfg)\n",
    "pipelines = { 'feature_pipe': feature_pipe, 'target_pipe': target_pipe }\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "# df_train = pipe.fit_transform(df_train)\n",
    "# df_test = pipe.transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "df_train = pd.concat([\n",
    "    pd.DataFrame(X_train, columns=feature_columns),\n",
    "    pd.DataFrame(y_train, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    pd.DataFrame(X_test, columns=feature_columns),\n",
    "    pd.DataFrame(y_test, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "# columns = [ 'feat_' + str(i) if i < df_train.shape[1]-len(pipe.transformer_list[2][1].named_steps['target_encode'].col_cats[0])-1 else 'target_' + str(i) for i in range(df_train.shape[1]) ]\n",
    "# df_train = pd.DataFrame(df_train, columns=columns)\n",
    "# df_test = pd.DataFrame(df_test, columns=columns)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# save train and test data\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "# save pipelines\n",
    "joblib.dump(pipelines, './outputs/pipelines.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SKLearn(entry_script='prep.py', source_directory='src', \n",
    "              inputs=[   ws.datasets['symptomcodes.csv'].as_named_input('symptomcodes'), \n",
    "                         ws.datasets['ItemResourceData.csv'].as_named_input('df')       ],\n",
    "              pip_packages=['pyarrow==0.12.0 '], compute_target='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: DummyPrediction_1592332738_1b42c70a\n",
      "Web View: https://ml.azure.com/experiments/DummyPrediction/runs/DummyPrediction_1592332738_1b42c70a?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-16T18:39:02.245224\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ prep.py ] with arguments: []\n",
      "After variable expansion, calling script [ prep.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.22268915176391602 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: DummyPrediction_1592332738_1b42c70a\n",
      "Web View: https://ml.azure.com/experiments/DummyPrediction/runs/DummyPrediction_1592332738_1b42c70a?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'DummyPrediction_1592332738_1b42c70a',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-16T18:39:01.553975Z',\n",
       " 'endTimeUtc': '2020-06-16T18:40:00.790316Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': '91a9da11-ab17-4883-bc8f-503222e13828'},\n",
       " 'inputDatasets': [{'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'df', 'mechanism': 'Direct'}}, {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'symptomcodes', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'prep.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'df': {'dataLocation': {'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'df',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False},\n",
       "   'symptomcodes': {'dataLocation': {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'symptomcodes',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment DummyPrediction Environment',\n",
       "   'version': 'Autosave_2020-06-16T09:24:13Z_984c6960',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['pyarrow==0.12.0',\n",
       "        'azureml-defaults',\n",
       "        'scikit-learn==0.20.3',\n",
       "        'scipy==1.2.1',\n",
       "        'numpy==1.16.2',\n",
       "        'joblib==0.13.2']}],\n",
       "     'name': 'azureml_6da8db82c0a6a27195a6a6ae29218268'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592332738_1b42c70a/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=qXDcyIJYH%2FDUjLGpzcP2fLNeghM0jReRRuBO7hE%2FCgI%3D&st=2020-06-16T18%3A30%3A01Z&se=2020-06-17T02%3A40%3A01Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592332738_1b42c70a/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=%2F73zv31nGIVvz22rdZW8S22043wHiUWxpi6NmwE9%2Bow%3D&st=2020-06-16T18%3A30%3A01Z&se=2020-06-17T02%3A40%3A01Z&sp=r',\n",
       "  'logs/azureml/8_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592332738_1b42c70a/logs/azureml/8_azureml.log?sv=2019-02-02&sr=b&sig=nGPa471iNz0%2Fvc1vm4R3IQd1EgzG2DZlJRsAyIYuiW0%3D&st=2020-06-16T18%3A30%3A01Z&se=2020-06-17T02%3A40%3A01Z&sp=r'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(ws, 'DummyPrediction')\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='resdynml1test', subscription_id='793146d9-d4dc-4a73-9728-76c4ffd0cc0d', resource_group='rg_dynamics_test'), name=DummyPipe, id=DummyPipe:4, version=4, tags={}, properties={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.register_model('DummyPipe', 'outputs/pipelines.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.Tabular.from_delimited_files(path, seperator, header, infer_column_types, set_column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import recall_score, precision_score, hamming_loss, zero_one_loss, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load data\n",
    "train_data = run.input_datasets['train_data'].to_pandas_dataframe()\n",
    "test_data = run.input_datasets['test_data'].to_pandas_dataframe()\n",
    "\n",
    "# split train/test and feat/target\n",
    "X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "\n",
    "############################################################\n",
    "\n",
    "# train classifier\n",
    "model = MultiOutputClassifier(DummyClassifier(strategy='stratified'))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate test data\n",
    "y_pred = model.predict(X_test)\n",
    "run.log('precision_macro', precision_score(y_test, y_pred, average='macro'))\n",
    "run.log('precision_samples', precision_score(y_test, y_pred, average='samples'))\n",
    "run.log('recall_macro', recall_score(y_test, y_pred, average='macro'))\n",
    "run.log('recall_samples', recall_score(y_test, y_pred, average='samples'))\n",
    "run.log('hamming_loss', hamming_loss(y_test, y_pred))\n",
    "run.log('zero_one_loss', zero_one_loss(y_test, y_pred))\n",
    "\n",
    "# evaluate train data\n",
    "y_pred = model.predict(X_train)\n",
    "run.log('precision_macro_train', precision_score(y_train, y_pred, average='macro'))\n",
    "run.log('precision_samples_train', precision_score(y_train, y_pred, average='samples'))\n",
    "run.log('recall_macro_train', recall_score(y_train, y_pred, average='macro'))\n",
    "run.log('recall_samples_train', recall_score(y_train, y_pred, average='samples'))\n",
    "run.log('hamming_loss_train', hamming_loss(y_train, y_pred))\n",
    "run.log('zero_one_loss_train', zero_one_loss(y_train, y_pred))\n",
    "\n",
    "# save model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "############################################################\n",
    "\n",
    "# train regressor\n",
    "X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]][['target_0']]\n",
    "X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]][['target_0']]\n",
    "\n",
    "model_regressor = DummyRegressor(strategy=\"mean\")\n",
    "model_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_regressor.predict(X_test)\n",
    "run.log('mae', mean_absolute_error(y_test, y_pred))\n",
    "run.log('mse', mean_squared_error(y_test, y_pred))\n",
    "run.log('r2', r2_score(y_test, y_pred))\n",
    "\n",
    "y_pred = model_regressor.predict(X_train)\n",
    "run.log('mae_train', mean_absolute_error(y_train, y_pred))\n",
    "run.log('mse_train', mean_squared_error(y_train, y_pred))\n",
    "run.log('r2_train', r2_score(y_train, y_pred))\n",
    "\n",
    "# save regressor model\n",
    "joblib.dump(value=model_regressor, filename='outputs/model_regressor.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SKLearn(entry_script='train.py', source_directory='src',\n",
    "             inputs=[    ws.datasets['DummyPredictionTrain'].as_named_input('train_data'), \n",
    "                         ws.datasets['DummyPredictionTest'].as_named_input('test_data')],\n",
    "             pip_packages=['pyarrow==0.12.0'], compute_target='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: DummyPrediction_1592385447_ac7e146e\n",
      "Web View: https://ml.azure.com/experiments/DummyPrediction/runs/DummyPrediction_1592385447_ac7e146e?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-17T09:17:29.657980\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ train.py ] with arguments: []\n",
      "After variable expansion, calling script [ train.py ] with arguments: []\n",
      "\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 8\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.4024679660797119 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: DummyPrediction_1592385447_ac7e146e\n",
      "Web View: https://ml.azure.com/experiments/DummyPrediction/runs/DummyPrediction_1592385447_ac7e146e?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'DummyPrediction_1592385447_ac7e146e',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-17T09:17:28.986177Z',\n",
       " 'endTimeUtc': '2020-06-17T09:23:06.75229Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': '8773d155-e70d-4dea-aaf0-b2b5dc53f383'},\n",
       " 'inputDatasets': [{'dataset': {'id': 'b217579b-16ba-4c90-9e95-1776ba592742'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'test_data', 'mechanism': 'Direct'}}, {'dataset': {'id': '723a8324-8b50-48ba-9398-53c659959f56'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'train_data', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'test_data': {'dataLocation': {'dataset': {'id': 'b217579b-16ba-4c90-9e95-1776ba592742',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'test_data',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False},\n",
       "   'train_data': {'dataLocation': {'dataset': {'id': '723a8324-8b50-48ba-9398-53c659959f56',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'train_data',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment DummyPrediction Environment',\n",
       "   'version': 'Autosave_2020-06-16T09:24:13Z_984c6960',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['pyarrow==0.12.0',\n",
       "        'azureml-defaults',\n",
       "        'scikit-learn==0.20.3',\n",
       "        'scipy==1.2.1',\n",
       "        'numpy==1.16.2',\n",
       "        'joblib==0.13.2']}],\n",
       "     'name': 'azureml_6da8db82c0a6a27195a6a6ae29218268'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592385447_ac7e146e/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=JLvhc5f2k2Gj0gIwp7ksNT38XLtwOFXO3IRm3OJDNTU%3D&st=2020-06-17T09%3A13%3A07Z&se=2020-06-17T17%3A23%3A07Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592385447_ac7e146e/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=i5aHnjlazH38XTkUhYMphRYS4Gklr12Z19W9dsSVQ0I%3D&st=2020-06-17T09%3A13%3A07Z&se=2020-06-17T17%3A23%3A07Z&sp=r',\n",
       "  'logs/azureml/8_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.DummyPrediction_1592385447_ac7e146e/logs/azureml/8_azureml.log?sv=2019-02-02&sr=b&sig=gmaROIcSFY9Vp2T5C9l%2BsqK4akud02Lx0%2Bu9iGzqhiE%3D&st=2020-06-17T09%3A13%3A07Z&se=2020-06-17T17%3A23%3A07Z&sp=r'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(ws, 'DummyPrediction')\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='resdynml1test', subscription_id='793146d9-d4dc-4a73-9728-76c4ffd0cc0d', resource_group='rg_dynamics_test'), name=DummyModelRegressor, id=DummyModelRegressor:4, version=4, tags={}, properties={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register trained model\n",
    "run.register_model('DummyModel', 'outputs/model.pkl')\n",
    "run.register_model('DummyModelRegressor', 'outputs/model_regressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./score/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./score/score.py\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "from pipe import create_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global regressor\n",
    "    global pipelines\n",
    "    model_path = Model.get_model_path('DummyModel')\n",
    "    model = joblib.load(model_path)\n",
    "    regressor_path = Model.get_model_path('DummyModelRegressor')\n",
    "    regressor = joblib.load(regressor_path)\n",
    "    pipeline_path = Model.get_model_path('DummyPipe')\n",
    "    pipelines = joblib.load(pipeline_path)\n",
    "    \n",
    "def run(raw_data):\n",
    "    \n",
    "    # get input data\n",
    "    data = json.loads(raw_data)\n",
    "    \n",
    "    # transform with pipeline\n",
    "    X = pipelines['feature_pipe'].transform(pd.DataFrame(data))\n",
    "    \n",
    "    # make prediction\n",
    "    y = model.predict(X)\n",
    "    \n",
    "    # predict duration\n",
    "    y_dur = regressor.predict(X)\n",
    "    \n",
    "    response = [\n",
    "        {\n",
    "            'Products':\n",
    "            [ \n",
    "                pipelines['target_pipe'].transformer_list[1][1].named_steps['target_encode'].col_cats[0][i] \n",
    "                for i in range(y.shape[1]) if y[j,i] == 1 \n",
    "            ],\n",
    "            'Duration':\n",
    "                 y_dur[j,0]\n",
    "        }        \n",
    "            for j in range(y.shape[0])\n",
    "    ]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\n",
      "Operation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\n",
      "More information can be found using '.get_logs()'\n",
      "Error:\n",
      "{\n",
      "  \"code\": \"ResourceNotFound\",\n",
      "  \"statusCode\": 404,\n",
      "  \"message\": \"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\"\n",
      "}\n",
      "\n",
      "ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\n",
      "Operation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\n",
      "More information can be found using '.get_logs()'\n",
      "Error:\n",
      "{\n",
      "  \"code\": \"ResourceNotFound\",\n",
      "  \"statusCode\": 404,\n",
      "  \"message\": \"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"ResourceNotFound\",\n  \"statusCode\": 404,\n  \"message\": \"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Transitioning\\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"ResourceNotFound\\\",\\n  \\\"statusCode\\\": 404,\\n  \\\"message\\\": \\\"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    676\u001b[0m                                           '{}'.format(self.state, self._operation_endpoint.split('/')[-1],\n\u001b[0;32m--> 677\u001b[0;31m                                                       logs_response, error_response), logger=module_logger)\n\u001b[0m\u001b[1;32m    678\u001b[0m             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"ResourceNotFound\",\n  \"statusCode\": 404,\n  \"message\": \"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Transitioning\\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"ResourceNotFound\\\",\\n  \\\"statusCode\\\": 404,\\n  \\\"message\\\": \\\"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\\\"\\n}\"\n    }\n}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b1d6dd18f074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                            deployment_config=aciconfig)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# print(service.state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    684\u001b[0m                                           'Current state is {}'.format(self.state), logger=module_logger)\n\u001b[1;32m    685\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_operation_to_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"ResourceNotFound\",\n  \"statusCode\": 404,\n  \"message\": \"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Transitioning\\nOperation ID: 63540ea1-71d8-4e20-abe2-ded689d406c3\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"ResourceNotFound\\\",\\n  \\\"statusCode\\\": 404,\\n  \\\"message\\\": \\\"The Resource 'Microsoft.ContainerInstance/containerGroups/dummy-product-prediction-aci' under resource group 'rg_dynamics_test' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice, LocalWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.add_pip_package('pyarrow==0.12.0')\n",
    "cd.add_pip_package('joblib')\n",
    "cd.add_pip_package('scikit-learn==0.20.3')\n",
    "cd.save_to_file(base_directory='./score', conda_file_path='myenv.yml')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"./score/myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv, source_directory='./score')\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                               auth_enabled=False, # this flag generates API keys to secure access\n",
    "                                               memory_gb=1,                                 \n",
    "                                               description='Dummy Classifier for Product Prediction')\n",
    "\n",
    "#aciconfig = LocalWebservice.deploy_configuration(port=1234)\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='dummy-product-prediction-aci',\n",
    "                           models= [ws.models['DummyModel'], ws.models['DummyModelRegressor'], ws.models['DummyPipe']], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment()\n",
    "# print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = ws.webservices['dummy-product-prediction-aci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-17T09:46:13,154070626+00:00 - gunicorn/run \n",
      "2020-06-17T09:46:13,155629335+00:00 - iot-server/run \n",
      "2020-06-17T09:46:13,159656457+00:00 - rsyslog/run \n",
      "2020-06-17T09:46:13,170818517+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2020-06-17T09:46:13,313812589+00:00 - iot-server/finish 1 0\n",
      "2020-06-17T09:46:13,315014495+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (11)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 38\n",
      "Initialized PySpark session.\n",
      "Initializing logger\n",
      "Starting up app insights client\n",
      "Starting up request id generator\n",
      "Starting up app insight hooks\n",
      "Invoking user's init function\n",
      "Users's init has completed successfully\n",
      "Scoring timeout is found from os.environ: 60000 ms\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [17/Jun/2020:09:46:30 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [17/Jun/2020:09:46:32 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [17/Jun/2020:09:54:48 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Validation Request Content-Type\n",
      "Received input: \r\n",
      "        [\r\n",
      "            {\"Symptoms\": [\"B001\", \"K001\"]}, \r\n",
      "            {\"Symptoms\": [\"B001\", \"K001\"]}\r\n",
      "        ]\r\n",
      "\n",
      "Headers passed in (total 14):\n",
      "\tHost: localhost:5001\n",
      "\tX-Real-Ip: 127.0.0.1\n",
      "\tX-Forwarded-For: unknown, 127.0.0.1\n",
      "\tX-Forwarded-Proto: http\n",
      "\tConnection: close\n",
      "\tContent-Length: 114\n",
      "\tUser-Agent: PostmanRuntime/7.6.0\n",
      "\tAccept: */*\n",
      "\tAccept-Encoding: gzip, deflate\n",
      "\tCache-Control: no-cache\n",
      "\tContent-Type: application/json\n",
      "\tPostman-Token: 14f26e37-7a49-4d2a-89f4-5152faa7778e\n",
      "\tVia: 1.1 orbproxy.orbis.de (squid/3.5.12)\n",
      "\tX-Ms-Request-Id: af4653f5-cdbc-4d85-a040-6dc86805d4de\n",
      "Scoring Timer is set to 60.0 seconds\n",
      "Encountered Exception: Traceback (most recent call last):\n",
      "  File \"/var/azureml-server/app.py\", line 221, in run_scoring\n",
      "    response = invoke_user_with_timer(service_input, request_headers)\n",
      "  File \"/var/azureml-server/app.py\", line 291, in invoke_user_with_timer\n",
      "    result = user_main.run(**params)\n",
      "  File \"/var/azureml-app/main.py\", line 21, in run\n",
      "    return_obj = driver_module.run(**arguments)\n",
      "  File \"/var/azureml-app/score/score.py\", line 46, in run\n",
      "    for j in range(y.shape[0])\n",
      "  File \"/var/azureml-app/score/score.py\", line 46, in <listcomp>\n",
      "    for j in range(y.shape[0])\n",
      "  File \"/var/azureml-app/score/score.py\", line 41, in <listcomp>\n",
      "    for i in range(y.shape[1]) if y[j,i] == 1\n",
      "KeyError: 'target_encode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/flask/app.py\", line 1832, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/azureml-envs/azureml_55f779a3e51294cd60bb1901cf850cf6/lib/python3.6/site-packages/flask/app.py\", line 1818, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/var/azureml-server/app.py\", line 138, in score_realtime\n",
      "    return run_scoring(service_input, request.headers, request.environ.get('REQUEST_ID', '00000000-0000-0000-0000-000000000000'))\n",
      "  File \"/var/azureml-server/app.py\", line 234, in run_scoring\n",
      "    raise RunFunctionException(str(exc))\n",
      "run_function_exception.RunFunctionException\n",
      "\n",
      "500\n",
      "127.0.0.1 - - [17/Jun/2020:09:55:18 +0000] \"POST /score HTTP/1.0\" 500 15 \"-\" \"PostmanRuntime/7.6.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ws.models['DummyPipe'].download(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "pipes = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes['target_pipe'].transformer_list[0][1].named_steps['target_encode'].col_cats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = { \n",
    "    'data' :\n",
    "        [\n",
    "            {'Symptoms': ['B001', 'K001']}, \n",
    "            {'Symptoms': ['B001', 'K001']}\n",
    "        ]\n",
    "}\n",
    "\n",
    "sample_output = {\n",
    "    'ProductNrs' : [ '0001', '0002' ]\n",
    "}\n",
    "\n",
    "sample_output = [['0001', '0002'], ['0001', '0002']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ws.models['DummyModel'].download(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pipe import create_pipeline\n",
    "model = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = pd.DataFrame(sample_input['data'])\n",
    "sample_input = pd.concat([pd.DataFrame(sample_input['Symptoms'].apply(lambda x: ' '.join(map(str,x))), columns=['Symptoms']),\n",
    "                          pd.DataFrame([0.0]*len(sample_input), columns=['duration']),\n",
    "                          pd.DataFrame(['']*len(sample_input), columns=['ProductNr'])], axis=1)\n",
    "\n",
    "x = pipe.transform(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'feat_' + str(i) if i < x.shape[1]-len(pipe.transformer_list[2][1].named_steps['target_encode'].col_cats[0])-1 else 'target_' + str(i) for i in range(x.shape[1]) ]\n",
    "x = pd.DataFrame(x, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[[ col for col in x.columns if col.startswith('feat')]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [ \n",
    "        pipe.transformer_list[2][1].named_steps['target_encode'].col_cats[0][i] \n",
    "        for i in range(y.shape[1]) if y[j,i] == 1 \n",
    "    ] \n",
    "    for j in range(y.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [ \n",
    "        pipe.transformer_list[2][1].named_steps['target_encode'].col_cats[0][i] \n",
    "        for i in range(y.shape[1]) if y[j,i] == 1 \n",
    "    ] \n",
    "    for j in range(y.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
