{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.estimator import Estimator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment cfg\n",
    "with open(\"experiment_cfg.json\", \"r\") as cfg_file:\n",
    "    cfg = json.load(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipeline(cfg):    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for target features\n",
    "    target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['target_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "#   # Pipeline for categories\n",
    "#     cat_pipe = Pipeline([\n",
    "#         ('cat_feature_select', DataFrameSelector(cfg['cat_cols'])),\n",
    "#         ('cat_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='0')),\n",
    "#         ('cat_one_hot_encode', OneHotEncoder(sparse=False))\n",
    "#     ])\n",
    "\n",
    "    # Pipeline for numericals\n",
    "    num_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_cols'], float))\n",
    "#         ('num_replace_missing', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "#         #('num_normalization', MinMaxScaler())\n",
    "#         ('num_standardization', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    feat_union = FeatureUnion([\n",
    "#         ('num_features', num_pipe),\n",
    "#         ('cat_features', cat_pipe),\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    pipeline = FeatureUnion([\n",
    "        (\"all_feat_pipe\", all_feat_pipe),\n",
    "        ('num_targets', num_pipe),\n",
    "        (\"target_pipe\", target_pipe)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def create_pipelines(cfg):\n",
    "    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # combine features\n",
    "    feat_union = FeatureUnion([\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    # preprocess all features\n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for multi target cols\n",
    "    multi_target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['multi_target_cols'], str)),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for numerical target cols\n",
    "    num_target_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_target_cols'], float))\n",
    "    ])\n",
    "    \n",
    "    all_target_pipe = FeatureUnion([\n",
    "        ('num_targets', num_target_pipe),\n",
    "        ('multi_targets', multi_target_pipe)\n",
    "    ])\n",
    "\n",
    "    return all_feat_pipe, all_target_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipelines\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "t = 0.5\n",
    "t_test = 0.1\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum())))),\n",
    "    'Start': x['Job Card.Date Start Work'].min(),\n",
    "    'End': x['Job Card.Date End Work'].max()\n",
    "  })).reset_index()\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame((df['End'] - df['Start']), columns=['duration'])],axis=1)\n",
    "df['duration'] = df['duration'].apply(lambda x: x.seconds / 3600)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['Symptoms']\n",
    "cfg['num_target_cols'] = ['duration']\n",
    "cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "feature_pipe, target_pipe = create_pipelines(cfg)\n",
    "pipelines = { 'feature_pipe': feature_pipe, 'target_pipe': target_pipe }\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "# df_train = pipe.fit_transform(df_train)\n",
    "# df_test = pipe.transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "df_train = pd.concat([\n",
    "    pd.DataFrame(X_train, columns=feature_columns),\n",
    "    pd.DataFrame(y_train, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "df_test = pd.concat([\n",
    "    pd.DataFrame(X_test, columns=feature_columns),\n",
    "    pd.DataFrame(y_test, columns=target_columns)\n",
    "], axis=1)\n",
    "\n",
    "# columns = [ 'feat_' + str(i) if i < df_train.shape[1]-len(pipe.transformer_list[2][1].named_steps['target_encode'].col_cats[0])-1 else 'target_' + str(i) for i in range(df_train.shape[1]) ]\n",
    "# df_train = pd.DataFrame(df_train, columns=columns)\n",
    "# df_test = pd.DataFrame(df_test, columns=columns)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# save train and test data\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "# save pipelines\n",
    "joblib.dump(pipelines, './outputs/pipelines.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SKLearn(entry_script='prep.py', source_directory='src', \n",
    "              inputs=[   ws.datasets['symptomcodes.csv'].as_named_input('symptomcodes'), \n",
    "                         ws.datasets['ItemResourceData.csv'].as_named_input('df')       ],\n",
    "              pip_packages=['pyarrow==0.12.0 '], compute_target='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'DummyPrediction')\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/prepared_data.csv', output_file_path='artifacts/prepared_data.csv')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/prepared_data.csv'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset.register(ws, cfg['prepared_data_dataset'], create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Pipeline (as Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model('DummyPipe', 'outputs/pipelines.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
