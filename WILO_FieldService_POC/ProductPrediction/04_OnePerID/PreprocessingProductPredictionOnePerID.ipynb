{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipeline(cfg):    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for target features\n",
    "    target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['target_cols'], str)),\n",
    "#         ('multi_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=tuple())),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "#   # Pipeline for categories\n",
    "#     cat_pipe = Pipeline([\n",
    "#         ('cat_feature_select', DataFrameSelector(cfg['cat_cols'])),\n",
    "#         ('cat_replace_missing', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='0')),\n",
    "#         ('cat_one_hot_encode', OneHotEncoder(sparse=False))\n",
    "#     ])\n",
    "\n",
    "#     # Pipeline for numericals\n",
    "#     num_pipe = Pipeline([\n",
    "#         ('num_feature_select', DataFrameSelector(cfg['num_cols'])),\n",
    "#         ('num_replace_missing', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "#         #('num_normalization', MinMaxScaler())\n",
    "#         ('num_standardization', StandardScaler())\n",
    "#     ])\n",
    "\n",
    "    feat_union = FeatureUnion([\n",
    "#         ('num_features', num_pipe),\n",
    "#         ('cat_features', cat_pipe),\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "        ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    pipeline = FeatureUnion([\n",
    "        (\"all_feat_pipe\", all_feat_pipe),\n",
    "        (\"target_pipe\", target_pipe)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "run.log('# rows before', len(df))\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "t = 5\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "run.log('# rows for last ' +str(t) + ' years', len(df))\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "run.log('# rows after cleaning', len(df))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "dfs = { n : g for n,g in df.groupby('Installed Base.InstalledBase ProductID')}\n",
    "\n",
    "# merge into one row per case\n",
    "dfs = { k : dfs[k].groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "#     'ProductGroup': tuple(x['Installed Base.Product Group'].unique()),\n",
    "    'ProductGroup': ' '.join(x['Installed Base.Product Group'].unique()),\n",
    "#     'ProductId': tuple(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'ProductId': ' '.join(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'Country': x['Location.Country'].unique()[0],\n",
    "    'City': x['Location.City'].unique()[0],\n",
    "    'LocationType': x['Location.Location Type'].unique()[0],\n",
    "    'PostalCode': x['Location.Postal Code'].unique()[0],\n",
    "#     'ProductName': tuple(x['Product.Product Name'].unique()), \n",
    "    'ProductName': ' '.join(x['Product.Product Name'].unique()), \n",
    "#     'ProductNr': tuple(x['Product.Product Number'].unique()),\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "#     'Quantity': tuple((x['Product.Product Number']),x['ItemResourceAppliedQuantity']),\n",
    "    'Start': x['Job Card.Date Start Work'].unique()[0],\n",
    "    'End': x['Job Card.Date End Work'].unique()[0],\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum()))))\n",
    "  })).reset_index() for k in dfs }\n",
    "\n",
    "dfs = { k : dfs[k] for k in dfs if len(dfs[k]) > 100}\n",
    "\n",
    "dfs_train = {}\n",
    "dfs_test = {}\n",
    "\n",
    "columns = ['CaseId','ProductGroup','ProductId','Country','City','LocationType','PostalCode','ProductName','ProductNr','Start','End','Symptoms']\n",
    "\n",
    "for k in dfs:\n",
    "    dfs_train[k] = pd.DataFrame(dfs[k].iloc[:-100].reset_index(drop=True).values.tolist(), columns=columns)\n",
    "    dfs_test[k] = pd.DataFrame(dfs[k].iloc[-100:].reset_index(drop=True).values.tolist(), columns=columns)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['ProductGroup', 'Symptoms'] #['ProductGroup', 'ProductId', 'Symptoms']\n",
    "cfg['cat_cols'] = ['Country', 'City', 'LocationType', 'PostalCode']\n",
    "cfg['date_cols'] = ['Start', 'End']\n",
    "cfg['num_cols'] = []\n",
    "cfg['target_cols'] = ['ProductNr']\n",
    "\n",
    "dfs_train_tr = {}\n",
    "dfs_test_tr = {}\n",
    "pipes = {}\n",
    "\n",
    "for k in dfs_train:\n",
    "    pipes[k] = create_pipeline(cfg)\n",
    "    dfs_train_tr[k] = pipes[k].fit_transform(dfs_train[k])\n",
    "    dfs_test_tr[k] = pipes[k].transform(dfs_test[k])\n",
    "    columns = [ 'feat_' + str(i) if i < dfs_train_tr[k].shape[1]-len(pipes[k].transformer_list[1][1].named_steps['target_encode'].col_cats[0]) else 'target_' + str(i) for i in range(dfs_train_tr[k].shape[1]) ]\n",
    "    dfs_train_tr[k] = pd.DataFrame(dfs_train_tr[k], columns=columns)\n",
    "    dfs_test_tr[k] = pd.DataFrame(dfs_test_tr[k], columns=columns)\n",
    "\n",
    "data = (dfs_train_tr, dfs_test_tr, pipes)    \n",
    "\n",
    "############################################################################\n",
    "\n",
    "# save prepared data to csv\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "# df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "joblib.dump(data, './outputs/data')\n",
    "\n",
    "############################################################################\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SKLearn(entry_script='prep.py', source_directory='src', \n",
    "              inputs=[   ws.datasets['symptomcodes.csv'].as_named_input('symptomcodes'), \n",
    "                         ws.datasets['ItemResourceData.csv'].as_named_input('df')       ],\n",
    "              pip_packages=['pyarrow==0.12.0 '], compute_target='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: ProductPredictionOnePerID_1592243223_7bd77c20\n",
      "Web View: https://ml.azure.com/experiments/ProductPredictionOnePerID/runs/ProductPredictionOnePerID_1592243223_7bd77c20?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-15T17:47:05.861226\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ prep.py ] with arguments: []\n",
      "After variable expansion, calling script [ prep.py ] with arguments: []\n",
      "\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/decomposition/pca.py:446: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "/azureml-envs/azureml_6da8db82c0a6a27195a6a6ae29218268/lib/python3.6/site-packages/sklearn/decomposition/pca.py:446: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 9\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.5570404529571533 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: ProductPredictionOnePerID_1592243223_7bd77c20\n",
      "Web View: https://ml.azure.com/experiments/ProductPredictionOnePerID/runs/ProductPredictionOnePerID_1592243223_7bd77c20?wsid=/subscriptions/793146d9-d4dc-4a73-9728-76c4ffd0cc0d/resourcegroups/rg_dynamics_test/workspaces/resdynml1test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'ProductPredictionOnePerID_1592243223_7bd77c20',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-15T17:47:04.998691Z',\n",
       " 'endTimeUtc': '2020-06-15T17:57:55.239555Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': '134d9377-c3e1-49ec-a49e-410d6cd7b528'},\n",
       " 'inputDatasets': [{'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'df', 'mechanism': 'Direct'}}, {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'symptomcodes', 'mechanism': 'Direct'}}],\n",
       " 'runDefinition': {'script': 'prep.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'df': {'dataLocation': {'dataset': {'id': '02e6cb83-4d0c-42b2-bbef-e103c74b3a3c',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'df',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False},\n",
       "   'symptomcodes': {'dataLocation': {'dataset': {'id': '88af5740-1a1b-4e09-8129-d3c538680909',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'symptomcodes',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment ProductPredictionOnePerID Environment',\n",
       "   'version': 'Autosave_2020-06-15T17:27:03Z_ce453b96',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['pyarrow==0.12.0',\n",
       "        'azureml-defaults',\n",
       "        'scikit-learn==0.20.3',\n",
       "        'scipy==1.2.1',\n",
       "        'numpy==1.16.2',\n",
       "        'joblib==0.13.2']}],\n",
       "     'name': 'azureml_6da8db82c0a6a27195a6a6ae29218268'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPredictionOnePerID_1592243223_7bd77c20/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=UkXqpGYz2uuy0HzrbQabnXCzvlfYcfL7bUXoWTquoLQ%3D&st=2020-06-15T17%3A47%3A56Z&se=2020-06-16T01%3A57%3A56Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPredictionOnePerID_1592243223_7bd77c20/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=x08QHUan%2FUJixdn8Y1lcAK%2BX7ktkp6Rj1uGe%2FPcG3MU%3D&st=2020-06-15T17%3A47%3A56Z&se=2020-06-16T01%3A57%3A56Z&sp=r',\n",
       "  'logs/azureml/9_azureml.log': 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPredictionOnePerID_1592243223_7bd77c20/logs/azureml/9_azureml.log?sv=2019-02-02&sr=b&sig=OefN36QmEI1faLBsc%2B9YMVdc9c22rW8Ka7%2FTXqbQTUI%3D&st=2020-06-15T17%3A47%3A56Z&se=2020-06-16T01%3A57%3A56Z&sp=r'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(ws, 'ProductPredictionOnePerID')\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials are not provided to access data from source. Please sign in using identity with required permission granted.\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CPMP5RKVQ to authenticate.\n"
     ]
    },
    {
     "ename": "ExecutionError",
     "evalue": "\nError Code: ScriptExecution.StreamAccess.Authentication\nFailed Step: 20e862f3-15f9-4f57-a294-aca69428a761\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by AuthenticationException.\n    Identity authentication failed for 'AzureBlob GetReference' operation at 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPredictionOnePerID_1592243223_7bd77c20/outputs/data' with '403: AuthenticationFailed'. Please make sure the compute or login identity has 'Storage Blob Data Reader' or 'Storage Blob Data Owner' role in the storage IAM.\n      Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n| session_id=ad4d3679-b067-42a3-a32f-0d092f46bb23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-801d0545efc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'oneperid_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_LoggerFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activity_info'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/file_dataset.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, target_path, overwrite)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         download_list = [os.path.abspath(os.path.join(target_path, '.' + p))\n\u001b[0;32m--> 123\u001b[0;31m                          for p in self._to_path(activity='download.to_path')]\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdownload_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/file_dataset.py\u001b[0m in \u001b[0;36m_to_path\u001b[0;34m(self, activity)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdataflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportable_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_portable_path_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdataflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataflow_for_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FileDataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_pyrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mportable_path\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/dataflow.py\u001b[0m in \u001b[0;36m_to_pyrecords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing_secrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             self._engine_api.execute_anonymous_activity(\n\u001b[0;32m--> 776\u001b[0;31m                 ExecuteAnonymousActivityMessageArguments(anonymous_activity=Dataflow._dataflow_to_anonymous_activity_data(dataflow_to_execute)))\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mintermediate_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintermediate_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'part-*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/engineapi/api.py\u001b[0m in \u001b[0;36mexecute_anonymous_activity\u001b[0;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mupdate_aml_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_engine_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_anonymous_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteAnonymousActivityMessageArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine.ExecuteActivity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/engineapi/engine.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                     \u001b[0mraise_engine_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmessage_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/errorhandlers.py\u001b[0m in \u001b[0;36mraise_engine_error\u001b[0;34m(error_response)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errorCode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'ScriptExecution'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mExecutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Validation'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.Authentication\nFailed Step: 20e862f3-15f9-4f57-a294-aca69428a761\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by AuthenticationException.\n    Identity authentication failed for 'AzureBlob GetReference' operation at 'https://resdynml1test6456542521.blob.core.windows.net/azureml/ExperimentRun/dcid.ProductPredictionOnePerID_1592243223_7bd77c20/outputs/data' with '403: AuthenticationFailed'. Please make sure the compute or login identity has 'Storage Blob Data Reader' or 'Storage Blob Data Owner' role in the storage IAM.\n      Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n| session_id=ad4d3679-b067-42a3-a32f-0d092f46bb23"
     ]
    }
   ],
   "source": [
    "# azureml-core 1.0.72 oder höher erforderlich\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '793146d9-d4dc-4a73-9728-76c4ffd0cc0d'\n",
    "resource_group = 'rg_dynamics_test'\n",
    "workspace_name = 'resdynml1test'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='oneperid_data')\n",
    "dataset.download(target_path='.', overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
