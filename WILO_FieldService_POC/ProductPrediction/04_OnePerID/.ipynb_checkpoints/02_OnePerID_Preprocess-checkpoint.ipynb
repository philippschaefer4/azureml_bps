{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, Dataset\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.estimator import Estimator\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment cfg\n",
    "with open(\"experiment_cfg.json\", \"r\") as cfg_file:\n",
    "    cfg = json.load(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/pipe.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names, dtype):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self        \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].astype(self.dtype).values\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, delimiter=None):\n",
    "        self.delimiter = delimiter\n",
    "    def fit(self, X, y=None):\n",
    "        self.col_cats = {}\n",
    "        for col in range(X.shape[1]):\n",
    "            cats = set()\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    for cat in X[row,col].split(self.delimiter):\n",
    "                        if not cat.strip() == '':\n",
    "                            cats.add(cat.strip())\n",
    "                else:\n",
    "                    cats.add(X[row,col])\n",
    "            self.col_cats[col] = list(cats)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            X_enc = np.zeros([X.shape[0], len(self.col_cats[col])])\n",
    "            for row in range(X.shape[0]):\n",
    "                if self.delimiter:\n",
    "                    cats = str(X[row,col]).split(self.delimiter)\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] in cats:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "                else:\n",
    "                    for col_cat_idx in range(len(self.col_cats[col])):\n",
    "                        if self.col_cats[col][col_cat_idx] == X[row,col]:\n",
    "                            X_enc[row, col_cat_idx] = 1\n",
    "            X_enc = np.array(X_enc)\n",
    "            X_tr.append(X_enc)\n",
    "        X_tr = np.concatenate(X_tr, axis=1)\n",
    "        return X_tr\n",
    "    \n",
    "def create_pipelines(cfg):\n",
    "    \n",
    "    # Pipeline for multilabel features\n",
    "    multi_pipe = Pipeline([\n",
    "        ('multi_feat_select', DataFrameSelector(cfg['multi_cols'], str)),\n",
    "        ('multi_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "    \n",
    "    # combine features\n",
    "    feat_union = FeatureUnion([\n",
    "        ('multi_features', multi_pipe)\n",
    "    ])\n",
    "    \n",
    "    # preprocess all features\n",
    "    all_feat_pipe = Pipeline([\n",
    "        ('all_features_pipe', feat_union),\n",
    "#         ('all_feautres_pca', PCA(n_components=0.8, svd_solver = 'full'))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for multi target cols\n",
    "    multi_target_pipe = Pipeline([\n",
    "        ('target_select', DataFrameSelector(cfg['multi_target_cols'], str)),\n",
    "        ('target_encode', MultiHotEncoder(delimiter=' '))\n",
    "    ])\n",
    "\n",
    "    # Pipeline for numerical target cols\n",
    "    num_target_pipe = Pipeline([\n",
    "        ('num_feature_select', DataFrameSelector(cfg['num_target_cols'], float))\n",
    "    ])\n",
    "    \n",
    "    all_target_pipe = FeatureUnion([\n",
    "        ('num_targets', num_target_pipe),\n",
    "        ('multi_targets', multi_target_pipe)\n",
    "    ])\n",
    "\n",
    "    return { 'feature_pipe': all_feat_pipe, 'target_pipe': all_target_pipe }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/preprocess.py\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pipe import create_pipelines\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--input', dest='prepared_data')\n",
    "parser.add_argument('--output', dest='preprocessed_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load datasets\n",
    "if args.prepared_data:\n",
    "    df = pd.read_csv(args.prepared_data + '/prepared_data.csv', sep=';', header=0)\n",
    "else:\n",
    "    df = run.input_datasets['df_prepared'].to_pandas_dataframe()\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # split data (test data from last t_test years)\n",
    "# t_test = 0.5\n",
    "# df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "# df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # select columns for training\n",
    "# cfg = {}\n",
    "# cfg['multi_cols'] = ['Symptoms']\n",
    "# cfg['num_target_cols'] = ['duration']\n",
    "# cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "# # create pipeline\n",
    "# pipelines = create_pipelines(cfg)\n",
    "\n",
    "# # fit pipelines and transform data\n",
    "# X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "# y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "# X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "# y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # rename columns\n",
    "# feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "# target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "# df_train = pd.concat([\n",
    "#     pd.DataFrame(X_train, columns=feature_columns),\n",
    "#     pd.DataFrame(y_train, columns=target_columns)\n",
    "# ], axis=1)\n",
    "\n",
    "# df_test = pd.concat([\n",
    "#     pd.DataFrame(X_test, columns=feature_columns),\n",
    "#     pd.DataFrame(y_test, columns=target_columns)\n",
    "# ], axis=1)\n",
    "\n",
    "# ##############################################################################\n",
    "\n",
    "# # save train and test data\n",
    "# path = args.preprocessed_data if args.preprocessed_data else './outputs'\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# df_train.to_csv(path + '/train_data.csv', sep=';', header=True, index=False)\n",
    "# df_test.to_csv(path + '/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "# # save pipelines\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "# joblib.dump(pipelines, './outputs/pipelines.pkl')\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['Symptoms']\n",
    "cfg['num_target_cols'] = ['duration']\n",
    "cfg['multi_target_cols'] = ['ProductNr']\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# split datasets into groups per productid (and keep only with more than n entries)\n",
    "n = 20\n",
    "df_prods = [ df[df['ProductId']==prod].reset_index(drop=True) for prod in df['ProductId'].unique() if len(df[df['ProductId']==prod])>=n]\n",
    "\n",
    "# prepare data for each productid and save in dictionary\n",
    "prod_pipes = {}\n",
    "prod_train_data = {}\n",
    "prod_test_data = {}\n",
    "\n",
    "for i in range(len(df_prods)):\n",
    "    \n",
    "    prodid = df_prods[i]['ProductId'][0]\n",
    "   \n",
    "    ##############################################################################\n",
    "\n",
    "    # train test split\n",
    "    # get n% newest rows as test\n",
    "    n = 25\n",
    "    df_train = df_prods[i].sort_values(by=['Start'], ascending=False).iloc[int(len(df_prods[i])*((n)/100)):]\n",
    "    df_test = df_prods[i].sort_values(by=['Start'], ascending=False).iloc[:int(len(df_prods[i])*((n)/100))]\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    # create pipeline\n",
    "    pipelines = create_pipelines(cfg)\n",
    "\n",
    "    # fit pipelines and transform data\n",
    "    X_train = pipelines['feature_pipe'].fit_transform(df_train)\n",
    "    y_train = pipelines['target_pipe'].fit_transform(df_train)\n",
    "    X_test = pipelines['feature_pipe'].transform(df_test)\n",
    "    y_test = pipelines['target_pipe'].transform(df_test)\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    # rename columns\n",
    "    feature_columns = [ 'feat_'+ str(i) for i in range(X_train.shape[1])]\n",
    "    target_columns = [ 'target_'+ str(i) for i in range(y_train.shape[1])]\n",
    "\n",
    "    ##############################################################################\n",
    "    \n",
    "    df_train = pd.concat([\n",
    "        pd.DataFrame(X_train, columns=feature_columns),\n",
    "        pd.DataFrame(y_train, columns=target_columns)\n",
    "    ], axis=1)\n",
    "\n",
    "    df_test = pd.concat([\n",
    "        pd.DataFrame(X_test, columns=feature_columns),\n",
    "        pd.DataFrame(y_test, columns=target_columns)\n",
    "    ], axis=1)\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    prod_pipes[prodid] = pipelines\n",
    "    prod_train_data[prodid] = df_train\n",
    "    prod_test_data[prodid] = df_test\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# save train and test data\n",
    "path = args.preprocessed_data if args.preprocessed_data else './outputs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "# df_train.to_csv(path + '/train_data.csv', sep=';', header=True, index=False)\n",
    "# df_test.to_csv(path + '/test_data.csv', sep=';', header=True, index=False)\n",
    "joblib.dump(prod_train_data, path + '/train_data.pkl')\n",
    "joblib.dump(prod_test_data, path + '/test_data.pkl')\n",
    "\n",
    "# save pipelines\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(prod_pipes, './outputs/pipelines.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(entry_script='preprocess.py', source_directory='src', \n",
    "              inputs=[ws.datasets[cfg['prepared_data_dataset']].as_named_input('df_prepared')],\n",
    "              compute_target='local', environment_definition=ws.environments[cfg['env_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, cfg['experiment_name'])\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/train_data.pkl', output_file_path='artifacts/train_data.pkl')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/train_data.pkl'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "#prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset = Dataset.File.from_files(path=data_ref)\n",
    "prepared_data_dataset.register(ws, cfg['train_dataset'], create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs/test_data.pkl', output_file_path='artifacts/test_data.pkl')\n",
    "ds = ws.datastores[cfg['storage']]\n",
    "data_ref = ds.upload_files(['artifacts/test_data.pkl'], target_path='./'+cfg['experiment_name'], overwrite=True)\n",
    "# prepared_data_dataset = Dataset.Tabular.from_delimited_files(data_ref, separator=';', header=True, infer_column_types=True)\n",
    "prepared_data_dataset = Dataset.File.from_files(path=data_ref)\n",
    "prepared_data_dataset.register(ws, cfg['test_dataset'], create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Pipelines (as Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model(cfg['PreprocessPipeline'], 'outputs/pipelines.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
