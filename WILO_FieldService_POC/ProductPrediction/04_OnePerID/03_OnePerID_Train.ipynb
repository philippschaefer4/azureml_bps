{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.train.estimator import Estimator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment cfg\n",
    "with open(\"experiment_cfg.json\", \"r\") as cfg_file:\n",
    "    cfg = json.load(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/train.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import os\n",
    "import joblib\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, hamming_loss, zero_one_loss, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--input', dest='preprocessed_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # load data\n",
    "# if args.preprocessed_data:\n",
    "#     train_data = pd.read_csv(args.preprocessed_data + '/train_data.csv', sep=';', header=0)\n",
    "#     test_data = pd.read_csv(args.preprocessed_data + '/test_data.csv', sep=';', header=0)\n",
    "# else:\n",
    "#     train_data = run.input_datasets['train_data'].to_pandas_dataframe()\n",
    "#     test_data = run.input_datasets['test_data'].to_pandas_dataframe()\n",
    "    \n",
    "# ############################################################\n",
    "\n",
    "# # split train/test and feat/target for classifier\n",
    "# X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "# y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "# X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "# y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]].drop(['target_0'], axis=1)\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # train classifier\n",
    "# model = MultiOutputClassifier(SGDClassifier())\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # evaluate test data\n",
    "# y_pred = model.predict(X_test).round()\n",
    "# run.log_table(\n",
    "#     'test_evaluation_classification',\n",
    "#     {\n",
    "#         'precision_macro': [precision_score(y_test, y_pred, average='macro')],\n",
    "#         'precision_samples': [precision_score(y_test, y_pred, average='samples')],\n",
    "#         'recall_macro': [recall_score(y_test, y_pred, average='macro')],\n",
    "#         'recall_samples': [recall_score(y_test, y_pred, average='samples')],\n",
    "#         'hamming_loss': [hamming_loss(y_test, y_pred)],\n",
    "#         'zero_one_loss': [zero_one_loss(y_test, y_pred)]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # evaluate train data\n",
    "# y_pred = model.predict(X_train).round()\n",
    "# run.log_table(\n",
    "#     'train_evaluation_classification',\n",
    "#     {\n",
    "#         'precision_macro_train': [precision_score(y_train, y_pred, average='macro')],\n",
    "#         'precision_samples_train': [precision_score(y_train, y_pred, average='samples')],\n",
    "#         'recall_macro_train': [recall_score(y_train, y_pred, average='macro')],\n",
    "#         'recall_samples_train': [recall_score(y_train, y_pred, average='samples')],\n",
    "#         'hamming_loss_train': [hamming_loss(y_train, y_pred)],\n",
    "#         'zero_one_loss_train': [zero_one_loss(y_train, y_pred)]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # save model\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "# joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # split train/test and feat/target for regressor\n",
    "# X_train = train_data[[ col for col in train_data.columns if col.startswith('feat')]]\n",
    "# y_train = train_data[[ col for col in train_data.columns if col.startswith('target')]][['target_0']]\n",
    "# X_test = test_data[[col for col in test_data.columns if col.startswith('feat')]]\n",
    "# y_test = test_data[[ col for col in test_data.columns if col.startswith('target')]][['target_0']]\n",
    "# y_test = y_test.values[:,0]\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # train regressor\n",
    "# model_regressor = SGDRegressor()\n",
    "# model_regressor.fit(X_train, y_train)\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # evaluate test data\n",
    "# y_pred = model_regressor.predict(X_test)\n",
    "# run.log_table(\n",
    "#     'test_evaluation_regression',\n",
    "#     {\n",
    "#         'mae': [mean_absolute_error(y_test, y_pred)],\n",
    "#         'mse': [mean_squared_error(y_test, y_pred)],\n",
    "#         'r2': [r2_score(y_test, y_pred)]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # evaluate train data\n",
    "# y_pred = model_regressor.predict(X_train)\n",
    "# run.log_table(\n",
    "#     'train_evaluation_regression',\n",
    "#     {\n",
    "#         'mae_train': [mean_absolute_error(y_train, y_pred)],\n",
    "#         'mse_train': [mean_squared_error(y_train, y_pred)],\n",
    "#         'r2_train': [r2_score(y_train, y_train)]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "# # save regressor model\n",
    "# joblib.dump(value=model_regressor, filename='outputs/model_regressor.pkl')\n",
    "\n",
    "############################################################\n",
    "\n",
    "# load data\n",
    "if args.preprocessed_data:\n",
    "    train_data = joblib.load(args.preprocessed_data + '/train_data.pkl')\n",
    "    test_data = joblib.load(args.preprocessed_data + '/test_data.pkl')\n",
    "else:\n",
    "    train_data = joblib.load(run.input_datasets['train_data'] + '/workspaceblobstore/SKLearnPrediction' + '/train_data.pkl')\n",
    "    test_data = joblib.load(run.input_datasets['test_data'] + '/workspaceblobstore/SKLearnPrediction' + '/test_data.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(entry_script='train.py', source_directory='src',\n",
    "                inputs=[ws.datasets[cfg['train_dataset']].as_named_input('train_data').as_mount(), \n",
    "                        ws.datasets[cfg['test_dataset']].as_named_input('test_data').as_mount()   ],\n",
    "                compute_target='local', environment_definition=ws.environments[cfg['env_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, cfg['experiment_name'])\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register trained model\n",
    "run.register_model(cfg['TrainedClassifier'], 'outputs/model.pkl')\n",
    "run.register_model(cfg['TrainedRegressor'], 'outputs/model_regressor.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
