{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"39288a38-ff19-432c-8011-1cd9d0dff445\")\n",
    "ws = Workspace(subscription_id=\"793146d9-d4dc-4a73-9728-76c4ffd0cc0d\", resource_group=\"rg_dynamics_test\", workspace_name=\"resdynml1test\", auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/prep.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = run.input_datasets['symptomcodes'].to_pandas_dataframe()\n",
    "df = run.input_datasets['df'].to_pandas_dataframe()\n",
    "\n",
    "run.log('# rows before', len(df))\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "t = 2\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "run.log('# rows for last ' +str(t) + ' years', len(df))\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "run.log('# rows after cleaning', len(df))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "#     'ProductGroup': tuple(x['Installed Base.Product Group'].unique()),\n",
    "    'ProductGroup': ' '.join(x['Installed Base.Product Group'].unique()),\n",
    "#     'ProductId': tuple(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'ProductId': ' '.join(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'Country': x['Location.Country'].unique()[0],\n",
    "    'City': x['Location.City'].unique()[0],\n",
    "    'LocationType': x['Location.Location Type'].unique()[0],\n",
    "    'PostalCode': x['Location.Postal Code'].unique()[0],\n",
    "#     'ProductName': tuple(x['Product.Product Name'].unique()), \n",
    "    'ProductName': ' '.join(x['Product.Product Name'].unique()), \n",
    "#     'ProductNr': tuple(x['Product.Product Number'].unique()),\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "#     'Quantity': tuple((x['Product.Product Number']),x['ItemResourceAppliedQuantity']),\n",
    "    'Start': x['Job Card.Date Start Work'].unique()[0],\n",
    "    'End': x['Job Card.Date End Work'].unique()[0],\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum()))))\n",
    "  })).reset_index()\n",
    "\n",
    "run.log('# rows after merging cases', len(df))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "t_test = 0.5\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "run.log('# rows in train data', len(df_train))\n",
    "run.log('# rows in test data', len(df_test))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['ProductGroup', 'Symptoms'] #['ProductGroup', 'ProductId', 'Symptoms']\n",
    "cfg['cat_cols'] = ['Country', 'City', 'LocationType', 'PostalCode']\n",
    "cfg['date_cols'] = ['Start', 'End']\n",
    "cfg['num_cols'] = []\n",
    "cfg['target_cols'] = ['ProductNr']\n",
    "\n",
    "# create pipeline\n",
    "pipe = create_pipeline(cfg)\n",
    "\n",
    "# transform data\n",
    "df_train = pipe.fit_transform(df_train)\n",
    "df_test = pipe.transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "columns = [ 'feat_' + str(i) if i < df_train.shape[1]-len(pipe.transformer_list[1][1].named_steps['target_encode'].col_cats[0]) else 'target_' + str(i) for i in range(df_train.shape[1]) ]\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_test = pd.DataFrame(df_test, columns=columns)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# save prepared data to csv\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pipe import create_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# load datasets\n",
    "df_symptoms = ws.datasets['symptomcodes.csv'].to_pandas_dataframe()\n",
    "df = ws.datasets['ItemResourceData.csv'].to_pandas_dataframe()\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# get only data from last t years\n",
    "t = 5\n",
    "df = df[df['Job Card.Date Start Work']>(datetime.datetime.today() - datetime.timedelta(days=t*365))]\n",
    "\n",
    "############################################################\n",
    "\n",
    "# clean data\n",
    "df = df.replace(['', '0', '-', '000','N/A'], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "run.log('# rows after cleaning', len(df))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# combine Component/Failure Code in train data\n",
    "df = pd.concat([df, pd.DataFrame(df.apply(lambda x: (x['Job Card.ComponentCode'],x['Job Card.FailureCode']), axis=1), columns=['CompFail'])], axis=1)\n",
    "\n",
    "# combine Component/Failure Code in symptom table\n",
    "df_symptoms = df_symptoms[['ComponentCode', 'FailureCode', 'Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']]\n",
    "df_symptoms = pd.concat([df_symptoms, pd.DataFrame(df_symptoms.apply(lambda x: (x['ComponentCode'],x['FailureCode']),axis=1), columns=['CompFail'])],axis=1)\n",
    "\n",
    "# merge train data on symptoms\n",
    "df = pd.merge(df, df_symptoms, on='CompFail', how='left')\n",
    "df = pd.concat([df, pd.DataFrame(df[['Symptom1', 'Symptom2', 'Symptom3', 'Symptom4']].apply(lambda x: tuple([ x[col] for col in ['Symptom1','Symptom2','Symptom3','Symptom4'] if str(x[col]) != 'None' ]), axis=1), columns=['Symptoms'])], axis=1)\n",
    "\n",
    "# merge into one row per case\n",
    "df = df.groupby('Job Card.JobCard Number').apply(lambda x: pd.Series({\n",
    "#     'ProductGroup': tuple(x['Installed Base.Product Group'].unique()),\n",
    "    'ProductGroup': ' '.join(x['Installed Base.Product Group'].unique()),\n",
    "#     'ProductId': tuple(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'ProductId': ' '.join(x['Installed Base.InstalledBase ProductID'].unique()),\n",
    "    'Country': x['Location.Country'].unique()[0],\n",
    "    'City': x['Location.City'].unique()[0],\n",
    "    'LocationType': x['Location.Location Type'].unique()[0],\n",
    "    'PostalCode': x['Location.Postal Code'].unique()[0],\n",
    "#     'ProductName': tuple(x['Product.Product Name'].unique()), \n",
    "    'ProductName': ' '.join(x['Product.Product Name'].unique()), \n",
    "#     'ProductNr': tuple(x['Product.Product Number'].unique()),\n",
    "    'ProductNr': ' '.join(x['Product.Product Number'].unique()),\n",
    "#     'Quantity': tuple((x['Product.Product Number']),x['ItemResourceAppliedQuantity']),\n",
    "    'Start': x['Job Card.Date Start Work'].unique()[0],\n",
    "    'End': x['Job Card.Date End Work'].unique()[0],\n",
    "    'Symptoms': ' '.join(map(str, list(set(x['Symptoms'].sum()))))\n",
    "  })).reset_index()\n",
    "\n",
    "run.log('# rows after merging cases', len(df))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# split data (test data from last t_test years)\n",
    "t_test = 0.5\n",
    "df_train = df[df['Start']<(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "df_test = df[df['Start']>=(datetime.datetime.today() - datetime.timedelta(days=t_test*365))]\n",
    "\n",
    "run.log('# rows in train data', len(df_train))\n",
    "run.log('# rows in test data', len(df_test))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# select columns for training\n",
    "cfg = {}\n",
    "cfg['multi_cols'] = ['ProductGroup', 'Symptoms'] #['ProductGroup', 'ProductId', 'Symptoms']\n",
    "cfg['cat_cols'] = ['Country', 'City', 'LocationType', 'PostalCode']\n",
    "cfg['date_cols'] = ['Start', 'End']\n",
    "cfg['num_cols'] = []\n",
    "cfg['target_cols'] = ['ProductNr']\n",
    "\n",
    "# create pipeline\n",
    "pipe = create_pipeline(cfg)\n",
    "\n",
    "# transform data\n",
    "df_train = pipe.fit_transform(df_train)\n",
    "df_test = pipe.transform(df_test)\n",
    "\n",
    "# rename columns\n",
    "columns = [ 'feat_' + str(i) if i < df_train.shape[1]-len(pipe.transformer_list[1][1].named_steps['target_encode'].col_cats[0]) else 'target_' + str(i) for i in range(df_train.shape[1]) ]\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_test = pd.DataFrame(df_test, columns=columns)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# save prepared data to csv\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "df_train.to_csv('./outputs/train_data.csv', sep=';', header=True, index=False)\n",
    "df_test.to_csv('./outputs/test_data.csv', sep=';', header=True, index=False)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "run.complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
